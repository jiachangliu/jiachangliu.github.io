<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Jiachang Liu </title> <meta name="author" content="Jiachang Liu"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jiachangliu.github.io/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Jiachang</span> Liu </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/prof_pic-480.webp 480w,/assets/img/prof_pic-800.webp 800w,/assets/img/prof_pic-1400.webp 1400w," type="image/webp" sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/prof_pic.jpg?f9a1b2cf8ee6859ce035d4c97468399b" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="prof_pic.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <p>fistName.lastName at cornell.edu</p> <p>Cornell University</p> <p>Ithaca, NY, 14853</p> </div> </div> <div class="clearfix"> <p>I am an assistant research professor (postdoc) at the Center for Data Science for Enterprise and Society (CDSES) at Cornell University. My hosts are Professor <a href="https://tech.cornell.edu/people/andrea-lodi/" rel="external nofollow noopener" target="_blank">Andrea Lodi</a> and Professor <a href="https://sorooshafiee.github.io/" rel="external nofollow noopener" target="_blank">Soroosh Shafiee</a>.</p> <p>Prior to joining Cornell, I completed my Ph.D. in Electrical and Computer Engineering at Duke University in 2024, advised by Professor <a href="https://users.cs.duke.edu/~cynthia/" rel="external nofollow noopener" target="_blank">Cynthia Rudin</a>. I also worked closely with Professor <a href="https://www.seltzer.com/margo/" rel="external nofollow noopener" target="_blank">Margo Seltzer</a>. Before Duke, I earned my B.S. degree with double majors in physics and mathematics and a minor in computer science from University of Michigan, Ann Arbor in 2018. Go Blue! Here is a <a href="https://drive.google.com/file/d/1apcXJB1xHcAuCrFvkBwUTjU37w9sWuhc/view?usp=share_link" rel="external nofollow noopener" target="_blank">link to my CV</a>.</p> <p>My research focuses on <em>building interpretable, trustworthy, and human-centered ML/AI systems</em> for high-stakes applications such as healthcare and scientific discovery. To achieve this, I develop <em>efficient and scalable optimization algorithms</em> that solve the challenging nonconvex and combinatorial problems at the <em>intersection of discrete and continuous optimization</em>. My work is organized around three key areas:</p> <ol> <li> <p><strong>Interpretable Model Creation:</strong> Creating highly accurate medical scoring systems and survival models that are simple enough to fit on an index card, enabling transparent and reliable decision-making;</p> </li> <li> <p><strong>Provably Optimal &amp; Scalable Solvers:</strong> Designing first-order, GPU-accelerated methods for <em>extreme sparse learning</em> that facilitate robust scientific discovery in fields such as nonlinear dynamical systems;</p> </li> <li> <p><strong>Human-Centered AI:</strong> Building interactive systems that leverage the <em>Rashomon Effect</em> — the existence of many good models — to facilitate seamless collaboration between domain experts and AI.</p> </li> </ol> </div> <h2> <a href="/news/" style="color: inherit">news</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Oct 25, 2025</th> <td> Hadis Anahideh, Adam Meyers, Hairong Wang, and I are organizing the <a href="https://sites.google.com/view/dmdaworkshop2025/home" rel="external nofollow noopener" target="_blank">The 20th INFORMS Workshop on Data Mining and Decision Analytics</a>, which will be held on October 25, 2025, in Atlanta, Georgia, USA, one day before the INFORMS Annual Meeting 2025. </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 15, 2025</th> <td> The work <a href="https://arxiv.org/abs/2502.09502" rel="external nofollow noopener" target="_blank">Scalable Optimal k-Sparse GLMs</a> has been selected as a finalist in the <a href="https://connect.informs.org/discussion/qsr-best-referred-paper-competition-finalists" rel="external nofollow noopener" target="_blank">2025 INFORMS Quality Statistics and Reliability (QSR) Best Paper Competition</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">May 09, 2025</th> <td> I was awarded the <a href="https://ece.duke.edu/about/awards/" rel="external nofollow noopener" target="_blank">Outstanding Dissertation Award</a> from the Duke ECE department. My dissertation can be found <a href="https://dukespace.lib.duke.edu/items/50920c18-41ae-4ccc-80c6-45544ad326bf" rel="external nofollow noopener" target="_blank">here</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 22, 2024</th> <td> I was awarded the runner-up award (2nd place) for the <a href="https://connect.informs.org/computing/awards/ics-student-paper-award" rel="external nofollow noopener" target="_blank">INFORMS Computing Society (ICS) Student Paper Award</a> for my paper <a href="https://arxiv.org/pdf/2304.06686" rel="external nofollow noopener" target="_blank">OKRidge</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 21, 2024</th> <td> I was awarded the runner-up award (2nd place) for the <a href="https://connect.informs.org/data-mining/awards/new-item22275866739965778" rel="external nofollow noopener" target="_blank">INFORMS Data Mining and Data Analysis (DMDA) Workshop Best Theoretical Paper</a> for my paper <a href="">FastSurvival</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 05, 2023</th> <td> Together with Cynthia Rudin, Margo Seltzer, and Chudi Zhong, I was awarded the <a href="https://cs.duke.edu/news/duke-cs-wins-second-place-bell-labs-2023-prize-competition" rel="external nofollow noopener" target="_blank">2023 Bell Labs Prize, 2nd place</a>, which recognizes game-changing innovations in science, technology, engineering, and mathematics. </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">HDSR</abbr> </div> <div id="liu2025userguided" class="col-sm-8"> <div class="title">User-Guided Interpretable Models: Rashomon Effect, Interaction, and Computation.</div> <div class="author"> <em>Jiachang Liu</em> and Cynthia Rudin </div> <div class="periodical"> <em>Harvard Data Science Review</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICML</abbr> </div> <div id="liu2025scalable" class="col-sm-8"> <div class="title">Scalable First-order Method for Certifying Optimal k-Sparse GLMs</div> <div class="author"> <em>Jiachang Liu</em>, Soroosh Shafiee, and Andrea Lodi </div> <div class="periodical"> <em>International Conference on Machine Learning (ICML)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2502.09502" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>This paper investigates the problem of certifying optimality for sparse generalized linear models (GLMs), where sparsity is enforced through an \ell_0 cardinality constraint. While branch-and-bound (BnB) frameworks can certify optimality by pruning nodes using dual bounds, existing methods for computing these bounds are either computationally intensive or exhibit slow convergence, limiting their scalability to large-scale problems. To address this challenge, we propose a first-order proximal gradient algorithm designed to solve the perspective relaxation of the problem within a BnB framework. Specifically, we formulate the relaxed problem as a composite optimization problem and demonstrate that the proximal operator of the non-smooth component can be computed exactly in log-linear time complexity, eliminating the need to solve a computationally expensive second-order cone program. Furthermore, we introduce a simple restart strategy that enhances convergence speed while maintaining low per-iteration complexity. Extensive experiments on synthetic and real-world datasets show that our approach significantly accelerates dual bound computations and is highly effective in providing optimality certificates for large-scale problems.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> </div> <div id="liu2024fastsurvival" class="col-sm-8"> <div class="title">FastSurvival: Hidden Computational Blessings in Training Cox Proportional Hazards Models</div> <div class="author"> <em>Jiachang Liu</em>, Rui Zhang, and Cynthia Rudin </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2410.19081" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Survival analysis is an important research topic with applications in healthcare, business, and manufacturing. One essential tool in this area is the Cox proportional hazards (CPH) model, which is widely used for its interpretability, flexibility, and predictive performance. However, for modern data science challenges such as high dimensionality (both n and p) and high feature correlations, current algorithms to train the CPH model have drawbacks, preventing us from using the CPH model at its full potential. The root cause is that the current algorithms, based on the Newton method, have trouble converging due to vanishing second order derivatives when outside the local region of the minimizer. To circumvent this problem, we propose new optimization methods by constructing and minimizing surrogate functions that exploit hidden mathematical structures of the CPH model. Our new methods are easy to implement and ensure monotonic loss decrease and global convergence. Empirically, we verify the computational efficiency of our methods. As a direct application, we show how our optimization methods can be used to solve the cardinality-constrained CPH problem, producing very sparse high-quality models that were not previously practical to construct. We list several extensions that our breakthrough enables, including optimization opportunities, theoretical questions on CPH’s mathematical structure, as well as other CPH-related applications.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICML</abbr> </div> <div id="RudinEtAlAmazing2024" class="col-sm-8"> <div class="title">Position: Amazing Things Come From Having Many Good Models</div> <div class="author"> Cynthia Rudin, Chudi Zhong<sup>*</sup>, Lesia Semenova<sup>*</sup>, Margo Seltzer<sup>*</sup>, Ronald Parr<sup>*</sup>, <em>Jiachang Liu<sup>*</sup></em>, Srikar Katta<sup>*</sup>, Jon Donnelly<sup>*</sup>, Harry Chen<sup>*</sup>, and Zachery Boner<sup>*</sup> </div> <div class="periodical"> <em>Proceedings of the International Conference on Machine Learning (ICML), Spotlight</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.mlr.press/v235/rudin24a.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>The \textitRashomon Effect, coined by Leo Breiman, describes the phenomenon that there exist many equally good predictive models for the same dataset. This phenomenon happens for many real datasets and when it does, it sparks both magic and consternation, but mostly magic. In light of the Rashomon Effect, this perspective piece proposes reshaping the way we think about machine learning, particularly for tabular data problems in the nondeterministic (noisy) setting. We address how the Rashomon Effect impacts (1) the existence of simple-yet-accurate models, (2) flexibility to address user preferences, such as fairness and monotonicity, without losing performance, (3) uncertainty in predictions, fairness, and explanations, (4) reliable variable importance, (5) algorithm choice, specifically, providing advanced knowledge of which algorithms might be suitable for a given problem, and (6) public policy. We also discuss a theory of when the Rashomon Effect occurs and why. Our goal is to illustrate how the Rashomon Effect can have a massive impact on the use of machine learning for complex problems in society.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> </div> <div id="liu2023okridge" class="col-sm-8"> <div class="title">OKRidge: Scalable Optimal k-Sparse Ridge Regression for Learning Dynamical Systems</div> <div class="author"> <em>Jiachang Liu</em>, Sam Rosen, Chudi Zhong, and Cynthia Rudin </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS), Spotlight</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2304.06686" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/jiachangliu/OKRidge" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We consider an important problem in scientific discovery, identifying sparse governing equations for nonlinear dynamical systems. This involves solving sparse ridge regression problems to provable optimality in order to determine which terms drive the underlying dynamics. We propose a fast algorithm, OKRidge, for sparse ridge regression, using a novel lower bound calculation involving, first, a saddle point formulation, and from there, either solving (i) a linear system or (ii) using an ADMM-based approach, where the proximal operators can be efficiently evaluated by solving another linear system and an isotonic regression problem. We also propose a method to warm-start our solver, which leverages a beam search. Experimentally, our methods attain provable optimality with run times that are orders of magnitude faster than those of the existing MIP formulations solved by the commercial solver Gurobi. </p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> </div> <div id="zhong2024exploring" class="col-sm-8"> <div class="title">Exploring and Interacting with the Set of Good Sparse Generalized Additive Models</div> <div class="author"> Chudi Zhong<sup>*</sup>, Zhi Chen<sup>*</sup>, <em>Jiachang Liu</em>, Margo Seltzer, and Cynthia Rudin </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/b1719f44953c2e0754a016ab267fe4e7-Paper-Conference.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>In real applications, interaction between machine learning models and domain experts is critical; however, the classical machine learning paradigm that usually produces only a single model does not facilitate such interaction. Approximating and exploring the Rashomon set, i.e., the set of all near-optimal models, addresses this practical challenge by providing the user with a searchable space containing a diverse set of models from which domain experts can choose. We present algorithms to efficiently and accurately approximate the Rashomon set of sparse, generalized additive models with ellipsoids for fixed support sets and use these ellipsoids to approximate Rashomon sets for many different support sets. The approximated Rashomon set serves as a cornerstone to solve practical challenges such as (1) studying the variable importance for the model class; (2) finding models under user-specified constraints (monotonicity, direct editing); and (3) investigating sudden changes in the shape functions. Experiments demonstrate the fidelity of the approximated Rashomon set and its effectiveness in solving practical challenges.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">NeurIPS</abbr> </div> <div id="liu2022fasterrisk" class="col-sm-8"> <div class="title">FasterRisk: Fast and Accurate Interpretable Risk Scores</div> <div class="author"> <em>Jiachang Liu<sup>*</sup></em>, Chudi Zhong<sup>*</sup>, Boxuan Li, Margo Seltzer, and Cynthia Rudin </div> <div class="periodical"> <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2210.05846" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/jiachangliu/FasterRisk" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Over the last century, risk scores have been the most popular form of predictive model used in healthcare and criminal justice. Risk scores are sparse linear models with integer coefficients; often these models can be memorized or placed on an index card. Typically, risk scores have been created either without data or by rounding logistic regression coefficients, but these methods do not reliably produce high-quality risk scores. Recent work used mathematical programming, which is computationally slow. We introduce an approach for efficiently producing a collection of high-quality risk scores learned from data. Specifically, our approach produces a pool of almost-optimal sparse continuous solutions, each with a different support set, using a beam-search algorithm. Each of these continuous solutions is transformed into a separate risk score through a "star ray" search, where a range of multipliers are considered before rounding the coefficients sequentially to maintain low logistic loss. Our algorithm returns all of these high-quality risk scores for the user to consider. This method completes within minutes and can be valuable in a broad variety of applications.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">AISTATS</abbr> </div> <div id="liu2022fast" class="col-sm-8"> <div class="title">Fast Sparse Classification for Generalized Linear and Additive Models</div> <div class="author"> <em>Jiachang Liu</em>, Chudi Zhong, Margo Seltzer, and Cynthia Rudin </div> <div class="periodical"> <em>In International Conference on Artificial Intelligence and Statistics</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://proceedings.mlr.press/v151/liu22f/liu22f.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/jiachangliu/fastSparse" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>We present fast classification techniques for sparse generalized linear and additive models. These techniques can handle thousands of features and thousands of observations in minutes, even in the presence of many highly correlated features. For fast sparse logistic regression, our computational speed-up over other best-subset search techniques owes to linear and quadratic surrogate cuts for the logistic loss that allow us to efficiently screen features for elimination, as well as use of a priority queue that favors a more uniform exploration of features. As an alternative to the logistic loss, we propose the exponential loss, which permits an analytical solution to the line search at each iteration. Our algorithms are generally 2 to 5 times faster than previous approaches. They produce interpretable models that have accuracy comparable to black box models on challenging datasets.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ACL-W</abbr> </div> <div id="liu2022makes" class="col-sm-8"> <div class="title">What Makes Good In-Context Examples for GPT-3?</div> <div class="author"> <em>Jiachang Liu</em>, Dinghan Shen, Yizhe Zhang, William B Dolan, Lawrence Carin, and Weizhu Chen </div> <div class="periodical"> <em>In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, Association for Computational Linguistics</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/abs/2101.06804" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/jiachangliu/KATEGPT3" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>GPT-3 has attracted lots of attention due to its superior performance across a wide range of NLP tasks, especially with its powerful and versatile in-context few-shot learning ability. Despite its success, we found that the empirical results of GPT-3 depend heavily on the choice of in-context examples. In this work, we investigate whether there are more effective strategies for judiciously selecting in-context examples (relative to random sampling) that better leverage GPT-3’s few-shot capabilities. Inspired by the recent success of leveraging a retrieval module to augment large-scale neural network models, we propose to retrieve examples that are semantically-similar to a test sample to formulate its corresponding prompt. Intuitively, the in-context examples selected with such a strategy may serve as more informative inputs to unleash GPT-3’s extensive knowledge. We evaluate the proposed approach on several natural language understanding and generation benchmarks, where the retrieval-based prompt selection approach consistently outperforms the random baseline. Moreover, it is observed that the sentence encoders fine-tuned on task-related datasets yield even more helpful retrieval results. Notably, significant gains are observed on tasks such as table-to-text generation (41.9% on the ToTTo dataset) and open-domain question answering (45.5% on the NQ dataset). We hope our investigation could help understand the behaviors of GPT-3 and large-scale pre-trained LMs in general and enhance their few-shot capabilities.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">TPAMI</abbr> </div> <div id="xu2022representing" class="col-sm-8"> <div class="title">Representing Graphs via Gromov-Wasserstein Factorization</div> <div class="author"> Hongteng Xu, <em>Jiachang Liu</em>, Dixin Luo, and Lawrence Carin </div> <div class="periodical"> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://ieeexplore.ieee.org/abstract/document/9720092" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>We propose a new nonlinear factorization model for graphs that have topological structures, and optionally, node attributes. This model is based on a pseudo-metric called Gromov-Wasserstein (GW) discrepancy, which compares graphs in a relational way. It estimates observed graphs as the GW barycenters constructed by a set of graph bases with different weights. By minimizing the difference between each observed graph and its GW barycenter-based estimation, we learn the graph bases and their weights associated with the observed graphs. The model achieves a novel and flexible factorization mechanism for graphs, in which both the observed graphs and the learnable graph bases can be unaligned and with different sizes. We design an effective approximate algorithm for learning this Gromov-Wasserstein factorization (GWF) model, unrolling loopy computations as stacked modules and computing gradients with backpropagation. For each observed graph, the weights lead to its permutation-invariant representation. These weights can be parametrized by a graph neural network so that we can extend the proposed transductive GWF model to an inductive version. Compared with state-of-the-art methods, our GWF method can represent graphs with better interpretability and using lower dimensionality, while simultaneously achieving encouraging results in graph clustering and classification task.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">ICML</abbr> </div> <div id="cheng2020club" class="col-sm-8"> <div class="title">CLUB: A Contrastive Log-ratio Upper Bound of Mutual Information</div> <div class="author"> Pengyu Cheng, Weituo Hao, Shuyang Dai, <em>Jiachang Liu</em>, Zhe Gan, and Lawrence Carin </div> <div class="periodical"> <em>In International Conference on Machine Learning</em>, 2020 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://proceedings.mlr.press/v119/cheng20b/cheng20b.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://github.com/Linear95/CLUB" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="abstract hidden"> <p>Mutual information (MI) minimization has gained considerable interests in various machine learning tasks. However, estimating and minimizing MI in high-dimensional spaces remains a challenging problem, especially when only samples, rather than distribution forms, are accessible. Previous works mainly focus on MI lower bound approximation, which is not applicable to MI minimization problems. In this paper, we propose a novel Contrastive Log-ratio Upper Bound (CLUB) of mutual information. We provide a theoretical analysis of the properties of CLUB and its variational approximation. Based on this upper bound, we introduce a MI minimization training scheme and further accelerate it with a negative sampling strategy. Simulation studies on Gaussian distributions show the reliable estimation ability of CLUB. Real-world MI minimization experiments, including domain adaptation and information bottleneck, demonstrate the effectiveness of the proposed method. The code is at https://github.com/Linear95/CLUB.</p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%6A%69%61%63%68%61%6E%67.%6C%69%75@%63%6F%72%6E%65%6C%6C.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://github.com/jiachangliu" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/jiachang-liu-891308a9" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://scholar.google.com/citations?user=W_Zyr0AAAAAJ&amp;hl" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://twitter.com/JiachangLiu_8" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> </div> <div class="contact-note"></div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jiachang Liu. Last updated: September 23, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>